{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MO61NwoY42D6",
        "outputId": "dd239a62-7fab-4461-8134-96709f4524eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,798 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,375 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,753 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,677 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,000 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\n",
            "Fetched 24.3 MB in 9s (2,846 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libice-dev librsvg2-common\n",
            "  libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libice-doc libsm-doc libxt-doc openjdk-8-demo openjdk-8-source visualvm libnss-mdns\n",
            "  fonts-nanum fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei\n",
            "  fonts-indic mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libice-dev librsvg2-common\n",
            "  libsm-dev libxkbfile1 libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless\n",
            "  openjdk-8-jre openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 22 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 50.1 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [30.8 MB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre amd64 8u442-b06~us1-0ubuntu1~22.04 [75.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [8,864 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk amd64 8u442-b06~us1-0ubuntu1~22.04 [4,077 kB]\n",
            "Fetched 50.1 MB in 7s (6,738 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 125044 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../04-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../06-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../07-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../08-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../09-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../10-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../11-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../12-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../13-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../14-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../15-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../16-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../17-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../18-openjdk-8-jre-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../19-openjdk-8-jre_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../20-openjdk-8-jdk-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../21-openjdk-8-jdk_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "openjdk version \"11.0.26\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y openjdk-8-jdk\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "\n",
        "!tar -xzf hadoop-3.3.6.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oYg7zcAr5IFt",
        "outputId": "0714c946-560f-48a9-a754-ade270690546"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-15 13:00:59--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M   110MB/s    in 8.4s    \n",
            "\n",
            "2025-03-15 13:01:27 (82.9 MB/s) - ‘hadoop-3.3.6.tar.gz’ saved [730107476/730107476]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "os.environ[\"PATH\"] = f\"{os.environ['HADOOP_HOME']}/bin:{os.environ['PATH']}\"\n",
        "\n",
        "!echo $JAVA_HOME\n",
        "!echo $HADOOP_HOME\n",
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alpr1AJu5Xwx",
        "outputId": "4ff8b514-faf2-4308-b3fc-50cf89ac4b06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64\n",
            "/content/hadoop-3.3.6\n",
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > $HADOOP_HOME/etc/hadoop/core-site.xml << 'EOL'\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "EOL\n",
        "\n",
        "cat > $HADOOP_HOME/etc/hadoop/hdfs-site.xml << 'EOL'\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.namenode.name.dir</name>\n",
        "        <value>/content/hdfs/namenode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgL2wKUl5a_K",
        "outputId": "d9dfc7b7-7be9-4f69-df7e-15acb4e9aa45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 22: warning: here-document at line 12 delimited by end-of-file (wanted `EOL')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start NameNode\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start namenode\n",
        "\n",
        "# Start DataNode\n",
        "!$HADOOP_HOME/bin/hdfs --daemon start datanode\n",
        "\n",
        "# Verifikasi daemons berjalan\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEcWX85M59Yj",
        "outputId": "6e6551ad-8ea9-4f7d-c646-e6839db76704"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /content/hadoop-3.3.6/logs does not exist. Creating.\n",
            "4752 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "  print(f'File yang terupload: {filename}')\n",
        "  print(f'Ukuran: {len(uploaded[filename])} bytes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "ME-4D6TY7sIK",
        "outputId": "fe45eaa2-cb48-4f56-9c96-5b61e34e9d0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-68f782b3-bb56-4187-9ffa-5e0f7bbddbd4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-68f782b3-bb56-4187-9ffa-5e0f7bbddbd4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wordcount.txt to wordcount.txt\n",
            "File yang terupload: wordcount.txt\n",
            "Ukuran: 1498 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat wordcount.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZOCBkr7zTK",
        "outputId": "cbb0853c-ac21-4587-d338-827bcff87fe9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNDANG-UNDANG DASAR NEGARA REPUBLIK INDONESIA TAHUN 1945\r\n",
            "PEMBUKAAN\r\n",
            "\r\n",
            "Bahwa sesungguhnya Kemerdekaan itu ialah hak segala bangsa dan oleh sebab itu, maka penjajahan di atas dunia harus dihapuskan, karena tidak sesuai dengan perikemanusiaan dan perikeadilan.\r\n",
            "\r\n",
            "\r\n",
            "Dan perjuangan pergerakan kemerdekaan Indonesia telah sampailah kepada saat yang berbahagia dengan selamat sentausa mengantarkan rakyat Indonesia ke depan pintu gerbang kemerdekaan Negara Indonesia, yang merdeka, bersatu, berdaulat, adil dan makmur.\r\n",
            "\r\n",
            "\r\n",
            "Atas berkat rakhmat Allah Yang Maha Kuasa dan dengan didorongkan oleh keinginan luhur, supaya berkehidupan kebangsaan yang bebas, maka rakyat Indonesia menyatakan dengan ini kemerdekaannya.\r\n",
            "\r\n",
            "\r\n",
            "Kemudian daripada itu untuk membentuk suatu Pemerintah Negara Indonesia yang melindungi segenap bangsa Indonesia dan seluruh tumpah darah Indonesia dan untuk memajukan kesejahteraan umum, mencerdaskan kehidupan bangsa, dan ikut melaksanakan ketertiban dunia yang berdasarkan kemerdekaan, perdamaian abadi dan keadilan sosial, maka disusunlah Kemerdekaan Kebangsaan Indonesia itu dalam suatu Undang-Undang Dasar Negara Indonesia, yang terbentuk dalam suatu susunan Negara Republik Indonesia yang berkedaulatan rakyat dengan berdasar kepada Ketuhanan Yang Maha Esa, Kemanusiaan yang adil dan beradab, Persatuan Indonesia dan Kerakyatan yang dipimpin oleh hikmat kebijaksanaan dalam Permusyawaratan/Perwakilan, serta dengan mewujudkan suatu Keadilan sosial bagi seluruh rakyat Indonesia."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "import re\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    line = line.lower()\n",
        "    line = re.sub(r'[^a-z0-9\\s]', ' ', line)\n",
        "    words = line.split()\n",
        "\n",
        "    for word in words:\n",
        "        print(f'{word}\\t1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOQ5Jman74Ll",
        "outputId": "6ae50e31-3f9d-4f4e-bc52-dacfba82c132"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    word, count = line.split('\\t', 1)\n",
        "\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        continue\n",
        "\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            print(f'{current_word}\\t{current_count}')\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "if current_word == word:\n",
        "    print(f'{current_word}\\t{current_count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHJL4oug8KSK",
        "outputId": "3230d3b8-564e-41da-8c37-ca62c0e48a31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x mapper.py\n",
        "!chmod +x reducer.py"
      ],
      "metadata": {
        "id": "Df4s20q-8NMP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "streaming_jar = f\"{os.environ['HADOOP_HOME']}/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\"\n",
        "\n",
        "!ls -la {streaming_jar}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um_3s2rr8b5G",
        "outputId": "840203b2-1cd0-473b-e321-9ce7c1e14c4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 1000 1000 141267 Jun 18  2023 /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find $HADOOP_HOME -name \"hadoop-streaming*.jar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9YrROdB8dYx",
        "outputId": "30e0a91a-9625-450a-9383-c755c458d38a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.3.6/share/hadoop/tools/sources/hadoop-streaming-3.3.6-test-sources.jar\n",
            "/content/hadoop-3.3.6/share/hadoop/tools/sources/hadoop-streaming-3.3.6-sources.jar\n",
            "/content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_jar = \"/content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\"\n",
        "\n",
        "!ls -la {streaming_jar}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "62pCDV088flQ",
        "outputId": "8ee27f12-3150-4f70-a9b3-8e315f6b0812"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 1000 1000 141267 Jun 18  2023 /content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -rm -r -f /user/colab/output_streaming\n",
        "\n",
        "!$HADOOP_HOME/bin/hadoop jar {streaming_jar} \\\n",
        "  -files mapper.py,reducer.py \\\n",
        "  -mapper \"python3 mapper.py\" \\\n",
        "  -reducer \"python3 reducer.py\" \\\n",
        "  -input /user/colab/input/wordcount.txt \\\n",
        "  -output /user/colab/output_streaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOOxTYQy9e_y",
        "outputId": "1e18c9e8-8c5c-4fbb-dc56-c2c511b332e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-15 13:19:51,302 ERROR conf.Configuration: error parsing conf hdfs-site.xml\n",
            "com.ctc.wstx.exc.WstxEOFException: Unexpected EOF; was expecting a close tag for element <value>\n",
            " at [row,col,system-id]: [11,0,\"file:/content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\"]\n",
            "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.throwUnexpectedEOF(BasicStreamReader.java:5590)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2791)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1121)\n",
            "\tat org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3405)\n",
            "\tat org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3191)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3084)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)\n",
            "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)\n",
            "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)\n",
            "\tat org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1301)\n",
            "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2687)\n",
            "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3573)\n",
            "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3608)\n",
            "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\n",
            "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
            "\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:347)\n",
            "\tat org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:264)\n",
            "\tat org.apache.hadoop.fs.shell.Delete$Rm.expandArgument(Delete.java:94)\n",
            "\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:247)\n",
            "\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)\n",
            "\tat org.apache.hadoop.fs.shell.Command.run(Command.java:191)\n",
            "\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:327)\n",
            "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\n",
            "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\n",
            "\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:390)\n",
            "-rm: Fatal internal error\n",
            "java.lang.RuntimeException: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF; was expecting a close tag for element <value>\n",
            " at [row,col,system-id]: [11,0,\"file:/content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\"]\n",
            "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3101)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3045)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2923)\n",
            "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2905)\n",
            "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1247)\n",
            "\tat org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1301)\n",
            "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2687)\n",
            "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3573)\n",
            "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3608)\n",
            "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n",
            "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n",
            "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\n",
            "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
            "\tat org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:347)\n",
            "\tat org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:264)\n",
            "\tat org.apache.hadoop.fs.shell.Delete$Rm.expandArgument(Delete.java:94)\n",
            "\tat org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:247)\n",
            "\tat org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:105)\n",
            "\tat org.apache.hadoop.fs.shell.Command.run(Command.java:191)\n",
            "\tat org.apache.hadoop.fs.FsShell.run(FsShell.java:327)\n",
            "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:82)\n",
            "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:97)\n",
            "\tat org.apache.hadoop.fs.FsShell.main(FsShell.java:390)\n",
            "Caused by: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF; was expecting a close tag for element <value>\n",
            " at [row,col,system-id]: [11,0,\"file:/content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\"]\n",
            "\tat com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:701)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.throwUnexpectedEOF(BasicStreamReader.java:5590)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.nextFromTree(BasicStreamReader.java:2791)\n",
            "\tat com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1121)\n",
            "\tat org.apache.hadoop.conf.Configuration$Parser.parseNext(Configuration.java:3405)\n",
            "\tat org.apache.hadoop.conf.Configuration$Parser.parse(Configuration.java:3191)\n",
            "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3084)\n",
            "\t... 25 more\n",
            "JAR does not exist or is not a normal file: /content/{streaming_jar}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADA ERROR MASALAH DISTREAMING JAR"
      ],
      "metadata": {
        "id": "ukvMwzVsA2r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRWZFym-9sLJ",
        "outputId": "e05c0d69-f7b8-45fc-a232-8f5c21846066"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<configuration>\n",
            "    <property>\n",
            "        <name>dfs.replication</name>\n",
            "        <value>1</value>\n",
            "    </property>\n",
            "    <property>\n",
            "        <name>dfs.namenode.name.dir</name>\n",
            "        <value>/content/hdfs/namenode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "hadoop_home = os.environ.get('HADOOP_HOME')\n",
        "print(f\"Nilai HADOOP_HOME: {hadoop_home}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osrnqwns96pW",
        "outputId": "1038450d-afe3-4830-be20-62972ece9491"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nilai HADOOP_HOME: /content/hadoop-3.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.namenode.name.dir</name>\n",
        "        <value>/content/hdfs/namenode</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.datanode.data.dir</name>\n",
        "        <value>/content/hdfs/datanode</value>\n",
        "    </property>\n",
        "</configuration>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xZcrSkK-Hmu",
        "outputId": "00c3eafb-5d3b-4122-9354-abc276926e91"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/hadoop-3.3.6/etc/hadoop/hdfs-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f \"hdfs\"\n",
        "!/content/hadoop-3.3.6/bin/hdfs --daemon stop datanode\n",
        "!/content/hadoop-3.3.6/bin/hdfs --daemon stop namenode\n",
        "\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "!mkdir -p /content/hdfs/namenode\n",
        "!mkdir -p /content/hdfs/datanode\n",
        "\n",
        "!/content/hadoop-3.3.6/bin/hdfs namenode -format -force\n",
        "\n",
        "!/content/hadoop-3.3.6/bin/hdfs --daemon start namenode\n",
        "!/content/hadoop-3.3.6/bin/hdfs --daemon start datanode\n",
        "\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bS7iM5BZ-KdL",
        "outputId": "961f653b-b815-48b8-b21e-5124c9f9bece"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-15 13:23:00,542 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 0c97970be51e/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -force]\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.3.6/etc/hadoop:/content/hadoop-3.3.6/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-kms-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/common/hadoop-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/avro-1.7.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/paranamer-2.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okio-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn:/content/hadoop-3.3.6/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-commons-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/json-io-2.5.1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/guice-4.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/java-util-1.9.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/asm-tree-9.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.3.6/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar:/content/hadoop-3.3.6/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c; compiled by 'ubuntu' on 2023-06-18T08:22Z\n",
            "STARTUP_MSG:   java = 1.8.0_442\n",
            "************************************************************/\n",
            "2025-03-15 13:23:00,585 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2025-03-15 13:23:00,774 INFO namenode.NameNode: createNameNode [-format, -force]\n",
            "2025-03-15 13:23:01,663 INFO common.Util: Assuming 'file' scheme for path /content/hdfs/namenode in configuration.\n",
            "2025-03-15 13:23:01,664 INFO common.Util: Assuming 'file' scheme for path /content/hdfs/namenode in configuration.\n",
            "2025-03-15 13:23:01,672 INFO namenode.NameNode: Formatting using clusterid: CID-f212bd2e-5678-4ba0-ad34-3a9e0dbf1001\n",
            "2025-03-15 13:23:01,761 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2025-03-15 13:23:01,822 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2025-03-15 13:23:01,825 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2025-03-15 13:23:01,828 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2025-03-15 13:23:01,840 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2025-03-15 13:23:01,840 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2025-03-15 13:23:01,841 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2025-03-15 13:23:01,841 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2025-03-15 13:23:01,841 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2025-03-15 13:23:01,930 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2025-03-15 13:23:02,163 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2025-03-15 13:23:02,163 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2025-03-15 13:23:02,195 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2025-03-15 13:23:02,196 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Mar 15 13:23:02\n",
            "2025-03-15 13:23:02,199 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2025-03-15 13:23:02,199 INFO util.GSet: VM type       = 64-bit\n",
            "2025-03-15 13:23:02,201 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2025-03-15 13:23:02,201 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2025-03-15 13:23:02,221 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2025-03-15 13:23:02,222 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2025-03-15 13:23:02,232 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2025-03-15 13:23:02,232 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2025-03-15 13:23:02,233 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2025-03-15 13:23:02,233 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2025-03-15 13:23:02,233 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2025-03-15 13:23:02,234 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2025-03-15 13:23:02,234 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2025-03-15 13:23:02,234 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2025-03-15 13:23:02,234 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2025-03-15 13:23:02,234 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2025-03-15 13:23:02,299 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2025-03-15 13:23:02,299 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2025-03-15 13:23:02,299 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2025-03-15 13:23:02,299 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2025-03-15 13:23:02,323 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2025-03-15 13:23:02,323 INFO util.GSet: VM type       = 64-bit\n",
            "2025-03-15 13:23:02,323 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2025-03-15 13:23:02,323 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2025-03-15 13:23:02,469 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2025-03-15 13:23:02,469 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2025-03-15 13:23:02,469 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2025-03-15 13:23:02,470 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2025-03-15 13:23:02,483 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2025-03-15 13:23:02,491 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2025-03-15 13:23:02,500 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2025-03-15 13:23:02,500 INFO util.GSet: VM type       = 64-bit\n",
            "2025-03-15 13:23:02,501 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2025-03-15 13:23:02,501 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2025-03-15 13:23:02,531 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2025-03-15 13:23:02,531 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2025-03-15 13:23:02,531 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2025-03-15 13:23:02,544 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2025-03-15 13:23:02,545 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2025-03-15 13:23:02,547 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2025-03-15 13:23:02,547 INFO util.GSet: VM type       = 64-bit\n",
            "2025-03-15 13:23:02,548 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2025-03-15 13:23:02,548 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2025-03-15 13:23:02,618 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1568516608-172.28.0.12-1742044982596\n",
            "2025-03-15 13:23:02,667 INFO common.Storage: Storage directory /content/hdfs/namenode has been successfully formatted.\n",
            "2025-03-15 13:23:02,957 INFO namenode.FSImageFormatProtobuf: Saving image file /content/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2025-03-15 13:23:03,136 INFO namenode.FSImageFormatProtobuf: Image file /content/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\n",
            "2025-03-15 13:23:03,153 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2025-03-15 13:23:03,203 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2025-03-15 13:23:03,204 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2025-03-15 13:23:03,212 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2025-03-15 13:23:03,212 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 0c97970be51e/172.28.0.12\n",
            "************************************************************/\n",
            "9785 NameNode\n",
            "9867 Jps\n",
            "9839 DataNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HLm3ZP9-a38",
        "outputId": "d1be1308-4b22-4baf-8391-8151c2ea05be"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10260 Jps\n",
            "9785 NameNode\n",
            "9839 DataNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_jar = \"/content/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\"\n",
        "\n",
        "!/content/hadoop-3.3.6/bin/hdfs dfs -rm -r -f /user/colab/output_streaming\n",
        "\n",
        "!/content/hadoop-3.3.6/bin/hadoop jar {streaming_jar} \\\n",
        "  -files mapper.py,reducer.py \\\n",
        "  -mapper \"python3 mapper.py\" \\\n",
        "  -reducer \"python3 reducer.py\" \\\n",
        "  -input /user/colab/input/wordcount.txt \\\n",
        "  -output /user/colab/output_streaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xZrSiQzd-hcO",
        "outputId": "5aaa85eb-9c28-4860-bc6b-41995c61763e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-15 13:24:52,541 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-03-15 13:24:52,815 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-03-15 13:24:52,815 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-03-15 13:24:52,862 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-03-15 13:24:53,569 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2025-03-15 13:24:53,718 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2025-03-15 13:24:54,157 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local215390605_0001\n",
            "2025-03-15 13:24:54,157 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-03-15 13:24:54,892 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local215390605_0001_620df2d1-3e0e-4f3e-9e5e-d38462ffc694/mapper.py\n",
            "2025-03-15 13:24:55,003 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local215390605_0001_ed02194a-af3f-435f-aecb-adf266ba3bef/reducer.py\n",
            "2025-03-15 13:24:55,239 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-03-15 13:24:55,245 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-03-15 13:24:55,245 INFO mapreduce.Job: Running job: job_local215390605_0001\n",
            "2025-03-15 13:24:55,261 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2025-03-15 13:24:55,282 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-03-15 13:24:55,282 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-03-15 13:24:55,386 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-03-15 13:24:55,391 INFO mapred.LocalJobRunner: Starting task: attempt_local215390605_0001_m_000000_0\n",
            "2025-03-15 13:24:55,453 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-03-15 13:24:55,458 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-03-15 13:24:55,495 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-03-15 13:24:55,510 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/colab/input/wordcount.txt:0+1498\n",
            "2025-03-15 13:24:55,554 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-03-15 13:24:55,697 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-03-15 13:24:55,697 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-03-15 13:24:55,697 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-03-15 13:24:55,697 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-03-15 13:24:55,697 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-03-15 13:24:55,706 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-03-15 13:24:55,722 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/python3, mapper.py]\n",
            "2025-03-15 13:24:55,747 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2025-03-15 13:24:55,755 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2025-03-15 13:24:55,756 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2025-03-15 13:24:55,756 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2025-03-15 13:24:55,757 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2025-03-15 13:24:55,761 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2025-03-15 13:24:55,769 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2025-03-15 13:24:55,771 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2025-03-15 13:24:55,771 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2025-03-15 13:24:55,772 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2025-03-15 13:24:55,772 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-03-15 13:24:55,773 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2025-03-15 13:24:56,465 INFO mapreduce.Job: Job job_local215390605_0001 running in uber mode : false\n",
            "2025-03-15 13:24:56,466 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2025-03-15 13:24:56,590 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-03-15 13:24:56,593 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-03-15 13:24:56,600 INFO streaming.PipeMapRed: Records R/W=13/1\n",
            "2025-03-15 13:24:56,615 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-03-15 13:24:56,616 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-03-15 13:24:56,619 INFO mapred.LocalJobRunner: \n",
            "2025-03-15 13:24:56,620 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-03-15 13:24:56,620 INFO mapred.MapTask: Spilling map output\n",
            "2025-03-15 13:24:56,620 INFO mapred.MapTask: bufstart = 0; bufend = 1838; bufvoid = 104857600\n",
            "2025-03-15 13:24:56,620 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213644(104854576); length = 753/6553600\n",
            "2025-03-15 13:24:56,641 INFO mapred.MapTask: Finished spill 0\n",
            "2025-03-15 13:24:56,662 INFO mapred.Task: Task:attempt_local215390605_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-03-15 13:24:56,668 INFO mapred.LocalJobRunner: Records R/W=13/1\n",
            "2025-03-15 13:24:56,668 INFO mapred.Task: Task 'attempt_local215390605_0001_m_000000_0' done.\n",
            "2025-03-15 13:24:56,679 INFO mapred.Task: Final Counters for attempt_local215390605_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142979\n",
            "\t\tFILE: Number of bytes written=787065\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1498\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13\n",
            "\t\tMap output records=189\n",
            "\t\tMap output bytes=1838\n",
            "\t\tMap output materialized bytes=2222\n",
            "\t\tInput split bytes=104\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=189\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=499\n",
            "\t\tTotal committed heap usage (bytes)=532152320\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1498\n",
            "2025-03-15 13:24:56,679 INFO mapred.LocalJobRunner: Finishing task: attempt_local215390605_0001_m_000000_0\n",
            "2025-03-15 13:24:56,680 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2025-03-15 13:24:56,687 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-03-15 13:24:56,688 INFO mapred.LocalJobRunner: Starting task: attempt_local215390605_0001_r_000000_0\n",
            "2025-03-15 13:24:56,701 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-03-15 13:24:56,704 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-03-15 13:24:56,705 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-03-15 13:24:56,714 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5b8331bc\n",
            "2025-03-15 13:24:56,717 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-03-15 13:24:56,755 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-03-15 13:24:56,770 INFO reduce.EventFetcher: attempt_local215390605_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-03-15 13:24:56,806 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local215390605_0001_m_000000_0 decomp: 2218 len: 2222 to MEMORY\n",
            "2025-03-15 13:24:56,814 INFO reduce.InMemoryMapOutput: Read 2218 bytes from map-output for attempt_local215390605_0001_m_000000_0\n",
            "2025-03-15 13:24:56,817 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2218, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2218\n",
            "2025-03-15 13:24:56,820 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-03-15 13:24:56,821 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-03-15 13:24:56,822 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-03-15 13:24:56,831 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-03-15 13:24:56,832 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2211 bytes\n",
            "2025-03-15 13:24:56,838 INFO reduce.MergeManagerImpl: Merged 1 segments, 2218 bytes to disk to satisfy reduce memory limit\n",
            "2025-03-15 13:24:56,839 INFO reduce.MergeManagerImpl: Merging 1 files, 2222 bytes from disk\n",
            "2025-03-15 13:24:56,840 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-03-15 13:24:56,840 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-03-15 13:24:56,841 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2211 bytes\n",
            "2025-03-15 13:24:56,841 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-03-15 13:24:56,849 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/python3, reducer.py]\n",
            "2025-03-15 13:24:56,852 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2025-03-15 13:24:56,855 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2025-03-15 13:24:56,932 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-03-15 13:24:56,932 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-03-15 13:24:56,934 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-03-15 13:24:56,971 INFO streaming.PipeMapRed: Records R/W=189/1\n",
            "2025-03-15 13:24:56,988 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-03-15 13:24:56,988 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-03-15 13:24:57,070 INFO mapred.Task: Task:attempt_local215390605_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-03-15 13:24:57,074 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-03-15 13:24:57,074 INFO mapred.Task: Task attempt_local215390605_0001_r_000000_0 is allowed to commit now\n",
            "2025-03-15 13:24:57,110 INFO output.FileOutputCommitter: Saved output of task 'attempt_local215390605_0001_r_000000_0' to hdfs://localhost:9000/user/colab/output_streaming\n",
            "2025-03-15 13:24:57,112 INFO mapred.LocalJobRunner: Records R/W=189/1 > reduce\n",
            "2025-03-15 13:24:57,113 INFO mapred.Task: Task 'attempt_local215390605_0001_r_000000_0' done.\n",
            "2025-03-15 13:24:57,113 INFO mapred.Task: Final Counters for attempt_local215390605_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=147455\n",
            "\t\tFILE: Number of bytes written=789287\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1498\n",
            "\t\tHDFS: Number of bytes written=1164\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=112\n",
            "\t\tReduce shuffle bytes=2222\n",
            "\t\tReduce input records=189\n",
            "\t\tReduce output records=112\n",
            "\t\tSpilled Records=189\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=532152320\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1164\n",
            "2025-03-15 13:24:57,114 INFO mapred.LocalJobRunner: Finishing task: attempt_local215390605_0001_r_000000_0\n",
            "2025-03-15 13:24:57,114 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-03-15 13:24:57,473 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2025-03-15 13:24:57,474 INFO mapreduce.Job: Job job_local215390605_0001 completed successfully\n",
            "2025-03-15 13:24:57,489 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=290434\n",
            "\t\tFILE: Number of bytes written=1576352\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=2996\n",
            "\t\tHDFS: Number of bytes written=1164\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13\n",
            "\t\tMap output records=189\n",
            "\t\tMap output bytes=1838\n",
            "\t\tMap output materialized bytes=2222\n",
            "\t\tInput split bytes=104\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=112\n",
            "\t\tReduce shuffle bytes=2222\n",
            "\t\tReduce input records=189\n",
            "\t\tReduce output records=112\n",
            "\t\tSpilled Records=378\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=499\n",
            "\t\tTotal committed heap usage (bytes)=1064304640\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1498\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1164\n",
            "2025-03-15 13:24:57,489 INFO streaming.StreamJob: Output directory: /user/colab/output_streaming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/hadoop-3.3.6/bin/hdfs dfs -ls /user/colab/output_streaming\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vEnmQAdI-r-x",
        "outputId": "6ebc5aa5-f3e9-4d51-d935-7674bf3037d4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2025-03-15 13:27 /user/colab/output_streaming/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup       1362 2025-03-15 13:27 /user/colab/output_streaming/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/hadoop-3.3.6/bin/hdfs dfs -cat /user/colab/output_streaming/part-00000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2TYEUq6c__Of",
        "outputId": "e17672b9-7939-4b2a-ef05-02e2c931acb3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1945\t1\n",
            "Allah\t1\n",
            "Atas\t1\n",
            "Bahwa\t1\n",
            "DASAR\t1\n",
            "Dan\t1\n",
            "Dasar\t1\n",
            "Esa,\t1\n",
            "INDONESIA\t1\n",
            "Indonesia\t9\n",
            "Indonesia,\t2\n",
            "Indonesia.\t1\n",
            "Keadilan\t1\n",
            "Kebangsaan\t1\n",
            "Kemanusiaan\t1\n",
            "Kemerdekaan\t2\n",
            "Kemudian\t1\n",
            "Kerakyatan\t1\n",
            "Ketuhanan\t1\n",
            "Kuasa\t1\n",
            "Maha\t2\n",
            "NEGARA\t1\n",
            "Negara\t4\n",
            "PEMBUKAAN\t1\n",
            "Pemerintah\t1\n",
            "Permusyawaratan/Perwakilan,\t1\n",
            "Persatuan\t1\n",
            "REPUBLIK\t1\n",
            "Republik\t1\n",
            "TAHUN\t1\n",
            "UNDANG-UNDANG\t1\n",
            "Undang-Undang\t1\n",
            "Yang\t2\n",
            "abadi\t1\n",
            "adil\t2\n",
            "atas\t1\n",
            "bagi\t1\n",
            "bangsa\t2\n",
            "bangsa,\t1\n",
            "bebas,\t1\n",
            "beradab,\t1\n",
            "berbahagia\t1\n",
            "berdasar\t1\n",
            "berdasarkan\t1\n",
            "berdaulat,\t1\n",
            "berkat\t1\n",
            "berkedaulatan\t1\n",
            "berkehidupan\t1\n",
            "bersatu,\t1\n",
            "dalam\t3\n",
            "dan\t10\n",
            "darah\t1\n",
            "daripada\t1\n",
            "dengan\t6\n",
            "depan\t1\n",
            "di\t1\n",
            "didorongkan\t1\n",
            "dihapuskan,\t1\n",
            "dipimpin\t1\n",
            "disusunlah\t1\n",
            "dunia\t2\n",
            "gerbang\t1\n",
            "hak\t1\n",
            "harus\t1\n",
            "hikmat\t1\n",
            "ialah\t1\n",
            "ikut\t1\n",
            "ini\t1\n",
            "itu\t3\n",
            "itu,\t1\n",
            "karena\t1\n",
            "ke\t1\n",
            "keadilan\t1\n",
            "kebangsaan\t1\n",
            "kebijaksanaan\t1\n",
            "kehidupan\t1\n",
            "keinginan\t1\n",
            "kemerdekaan\t2\n",
            "kemerdekaan,\t1\n",
            "kemerdekaannya.\t1\n",
            "kepada\t2\n",
            "kesejahteraan\t1\n",
            "ketertiban\t1\n",
            "luhur,\t1\n",
            "maka\t3\n",
            "makmur.\t1\n",
            "melaksanakan\t1\n",
            "melindungi\t1\n",
            "memajukan\t1\n",
            "membentuk\t1\n",
            "mencerdaskan\t1\n",
            "mengantarkan\t1\n",
            "menyatakan\t1\n",
            "merdeka,\t1\n",
            "mewujudkan\t1\n",
            "oleh\t3\n",
            "penjajahan\t1\n",
            "perdamaian\t1\n",
            "pergerakan\t1\n",
            "perikeadilan.\t1\n",
            "perikemanusiaan\t1\n",
            "perjuangan\t1\n",
            "pintu\t1\n",
            "rakhmat\t1\n",
            "rakyat\t4\n",
            "saat\t1\n",
            "sampailah\t1\n",
            "sebab\t1\n",
            "segala\t1\n",
            "segenap\t1\n",
            "selamat\t1\n",
            "seluruh\t2\n",
            "sentausa\t1\n",
            "serta\t1\n",
            "sesuai\t1\n",
            "sesungguhnya\t1\n",
            "sosial\t1\n",
            "sosial,\t1\n",
            "suatu\t4\n",
            "supaya\t1\n",
            "susunan\t1\n",
            "telah\t1\n",
            "terbentuk\t1\n",
            "tidak\t1\n",
            "tumpah\t1\n",
            "umum,\t1\n",
            "untuk\t2\n",
            "yang\t9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('output_word_count.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pkSbnjU5AFKe",
        "outputId": "e96a676c-d2e0-4575-a71f-22e7642036e6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9adc4484-0d6a-4a91-989e-c50e985c4bf9\", \"output_word_count.txt\", 1362)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('mapper.py')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('reducer.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "utdwXi8zCtng",
        "outputId": "81097c2e-221d-45ab-984c-c76f9a0a5b71"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e2aab7ac-b9fc-4997-b6f1-afe9eb9fa387\", \"mapper.py\", 495)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_57ba5388-f9fd-4915-9710-3578422b8086\", \"reducer.py\", 927)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}